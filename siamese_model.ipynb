{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597138007268",
   "display_name": "Python 3.8.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "\n",
    "Released under the CC BY 4.0 License (https://creativecommons.org/licenses/by/4.0/)\n",
    "\n",
    "Part of the code was inspired by the material presented [here](https://www.coursera.org/learn/sequence-models-in-nlp). Some of the material shown here is related to an assignment for that course. To avoid any ethical and legal problem, some of the code (e.g. the implemented loss function) will not be shown in its entirety.\n",
    "\n",
    "# Purpose of this notebook\n",
    "\n",
    "The purpose of this notebook is to present the implementation I prepared for the task of identifiying if two short texts are related. For example, one can train a model to identify if two questions are duplicates of one another, or if two tweets are likely to come from the same author. The first task was the motivator for an assignment of the course cited before, while the second task is one that is motivating my learning process. The presented implementation uses TensorFlow 2.3.0.\n",
    "\n",
    "# Intro\n",
    "\n",
    "The model is structured as a siamese model, where the input is composed of two separate instances of the texts to be analyzed. In this specific model, the similarity measure between input instances is implicitly given by the structure of the batches passed to the model (more on this optimization later on). The labels that we care about (e.g. question topic, tweet author) are not passed to the model. In this way we fit a generic model that, in principle, can predict the similarity of two inputs even if the model did not encounter that particular author or question topic at training time.\n",
    "\n",
    "# Features\n",
    "\n",
    "The input features for the model are represented by the sequences of the tokenized input texts. The details of the implementation of how these sequences are produced is not the focus of this notebook, however an example based on the tweet author identification task is presented later on.\n",
    "\n",
    "# Model definition\n",
    "The model is a quite simple example of a siamese model, where a sequence of layers treats each input simultaneously. The resulting embeddings are then concatenated to form the acutal model output, from which one can compute the similarity between the inputs. It is important to remark that both inputs are treated using the same parameter values, since the layer instances are the same.\n",
    "\n",
    "The core of the model is represented by the siamese sequential part, composed of:\n",
    "1. an embedding layer\n",
    "2. a bidirectional LSTM sequence of cells\n",
    "3. a flattening layer\n",
    "\n",
    "The last layer simply concatenates the results obtained by the sequential part, applied to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base imports\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "#to avoid GPU-related problems when using masking features\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from tensorflow.keras import Input, layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english') \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_dimension=32, sequence_length=sequence_length):\n",
    "\n",
    "    a_sequence = Input(shape=(None, 1 ), name=\"input_a_sequence\", dtype=tf.float32)\n",
    "\n",
    "    b_sequence = Input(shape=(None, 1 ), name=\"input_b_sequence\", dtype=tf.float32)\n",
    "\n",
    "    lstm_seq = tf.keras.Sequential()\n",
    "    lstm_seq.add(layers.Embedding(n_tokens, model_dimension, input_length=sequence_length, mask_zero=True))\n",
    "    lstm_seq.add(layers.Bidirectional(layers.LSTM(model_dimension)))\n",
    "    lstm_seq.add(layers.Flatten())\n",
    "\n",
    "    a_branch = lstm_seq(a_sequence)\n",
    "\n",
    "    b_branch = lstm_seq(b_sequence)\n",
    "\n",
    "\n",
    "    inputs = [\n",
    "        a_sequence,\n",
    "        b_sequence\n",
    "    ]\n",
    "\n",
    "    concatenated_branches_layer = layers.Concatenate(axis=1, name=\"branches\")([a_branch, b_branch])\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[concatenated_branches_layer]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch structure and loss function definition\n",
    "\n",
    "The basic idea is to use a triplet loss function that extends the relation between input entries to the entire batch of data. In essence, if $A, B$ are two input batches of shape $(batch\\_size, sequence\\_size)$ we want all entries at the same row position to be positive examples, and all possible pairs in the same input batch to be negative examples. We want to build batches where the resulting representations of similar examples are much closer than the resulting representations of dissimilar examples: $d(repr(A^{(j)}), repr(B^{(j)})) + \\alpha < d(repr(A^{(j)}), repr(B^{(k)}))$ where $k \\neq j$ and $A^{(j)}, B^{(j)}, B^{(k)}$ are the anchor, positive and negative examples and $\\alpha$ is a margin constant (an hyperparameter of the model) that pushes the model to learn to actually represent different instances in a different way.\n",
    "\n",
    "If batches are built this way, we can define a loss function that takes into consideration all possible pairs in a batch. We can measure the similarity between two of these resulting embeddings using the cosine similarity metric, and produce a matrix of cosine similarities for all possible pairs in the batch. The function described in [week4](https://www.coursera.org/learn/sequence-models-in-nlp) uses this matrix to measure an average loss computed using the similarities of the positive pairs and the negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def get_loss_function(alpha=0.25, batch_size=64):\n",
    "\n",
    "    identity_matrix = tf.eye(batch_size)\n",
    "\n",
    "    def _loss_fn(y_true, y_pred):\n",
    "        \"\"\"\n",
    "            y_pred: the data computed by the last layer of the model.\n",
    "                    In my tf2 model, the last layer concatenates the two resulting embeddings\n",
    "        \"\"\"\n",
    "        branch_size = y_pred.shape[1] // 2\n",
    "        a_branch = y_pred[:, 0:branch_size]\n",
    "        b_branch = y_pred[:, branch_size:]\n",
    "        ###\n",
    "        # the rest of the implementation is mostly identical to the \n",
    "        # loss function that has to be implemented in [week4]\n",
    "        # apart from the different modules in which helper functions\n",
    "        # can be found: np, tf.keras.backend and tf.linalg instead of\n",
    "        # the fastnp implementations in trax\n",
    "        ###\n",
    "        loss = None\n",
    "        return loss\n",
    "    return _loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator for the tweet author identification task\n",
    "\n",
    "The following utility functions are used by the generator function to build the batched inputs. Using these functions, the generator transforms the input sentences by preproceessing them with stop-word filtering, stemming and transforming them in a sequence of indexes using a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer(df, n_tokens=100):\n",
    "\n",
    "    token_generator = TweetTokenizer(\n",
    "        preserve_case=False,\n",
    "        reduce_len=False,\n",
    "        strip_handles=False\n",
    "    )\n",
    "\n",
    "    texts = [token_generator.tokenize(normalize_text(text)) for text in df[\"text\"].values]\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token=\"OOV\", num_words=n_tokens)\n",
    "\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \").lower()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r'[,!?;-]+', '.', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "    text = re.sub(r\"@[\\w\\-\\d]+\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"#([\\w\\-\\d]+)\", \"\\\\1\", text)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    premade_tokenizer = TweetTokenizer(preserve_case=False)\n",
    "\n",
    "    filtered_words = []\n",
    "\n",
    "    for word in premade_tokenizer.tokenize(text):\n",
    "        if(\n",
    "            word not in stopwords_english\n",
    "            and word not in string.punctuation\n",
    "        ):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            filtered_words.append(stem_word)\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def get_normal_tokenized_text_sequence(tokenizer, text, length):\n",
    "    oov_index = tokenizer.texts_to_sequences(['OOV'])[0][0]\n",
    "\n",
    "    tokenized_sequence = tokenizer.texts_to_sequences([text])[0][0:length]\n",
    "\n",
    "    if(len(tokenized_sequence) < length):\n",
    "        tokenized_sequence.extend([oov_index] * (length - len(tokenized_sequence)))\n",
    "\n",
    "    remapped_oov_index = 0\n",
    "\n",
    "    # remapping the default OOV index (1) to 0, to take advantage of the masking capability\n",
    "    # of the Embedding layer\n",
    "    tokenized_sequence = [(v if v != oov_index else remapped_oov_index) for v in tokenized_sequence]\n",
    "\n",
    "    return tokenized_sequence\n",
    "\n",
    "\n",
    "def get_instance_features(text_a, text_b, tokenizer, sequence_length):\n",
    "    text_a = normalize_text(text_a)\n",
    "\n",
    "    text_b = normalize_text(text_b)\n",
    "\n",
    "    instance_features = {\n",
    "        \"input_a_sequence\": get_normal_tokenized_text_sequence(tokenizer, text_a, length=sequence_length),\n",
    "        \"input_b_sequence\": get_normal_tokenized_text_sequence(tokenizer, text_b, length=sequence_length),\n",
    "    }\n",
    "\n",
    "    return instance_features\n",
    "\n",
    "\n",
    "def get_example_dataframe():\n",
    "    data = [\n",
    "        {'handle': 'h4', 'text': 'hey this is a completely unrelated text 0'},\n",
    "        {'handle': 'h4', 'text': 'hey this is a completely unrelated text 1'},\n",
    "        {'handle': 'h4', 'text': 'hey this is a completely unrelated text 2'},\n",
    "        {'handle': 'h3', 'text': 'another handle is writing 0'},\n",
    "        {'handle': 'h3', 'text': 'another handle is writing 1'},\n",
    "        {'handle': 'h3', 'text': 'another handle is writing 2'},\n",
    "        {'handle': 'h2', 'text': 'a very different type of text 0'},\n",
    "        {'handle': 'h2', 'text': 'a very different type of text 1'},\n",
    "        {'handle': 'h2', 'text': 'a very different type of text 2'},\n",
    "        {'handle': 'h1', 'text': 'this is a test tweet 0'},\n",
    "        {'handle': 'h1', 'text': 'this is a test tweet 1'},\n",
    "        {'handle': 'h1', 'text': 'this is a test tweet 2'}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_random_tweet_for_handle(df, handle, other_text=None):\n",
    "\n",
    "    go_on = True\n",
    "    max_attempts = 10\n",
    "    attempts = 0\n",
    "    while(go_on):\n",
    "        result = normalize_text(df[df[\"handle\"] == handle][\"text\"].sample().iloc[0])\n",
    "        attempts += 1\n",
    "        go_on = (\n",
    "            result in [None, '']\n",
    "            and other_text is not None \n",
    "            and result == other_text \n",
    "            and attempts <= max_attempts\n",
    "        )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a possible implementation of the generator function that can be used when training the model. The returned value is a batch consisting of the sequences of indexes corresponding to the tokenized inputs. Note that the second element of the returned pair, which represents the $y\\_true$ predictions, is not actually used by the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_siamese_network(batch_size=4, sequence_length=5):\n",
    "    df = get_example_dataframe()\n",
    "\n",
    "    tokenizer = init_tokenizer(df)\n",
    "\n",
    "    features = {\n",
    "        \"input_a_sequence\": [],\n",
    "        \"input_b_sequence\": []\n",
    "    }\n",
    "\n",
    "    output = np.full((batch_size, batch_size), -1)\n",
    "    np.fill_diagonal(output, 1)\n",
    "\n",
    "    while(True):\n",
    "        features = {\n",
    "            \"input_a_sequence\": [],\n",
    "            \"input_b_sequence\": []\n",
    "        }\n",
    "\n",
    "        #getting a list of unique authors (handles)\n",
    "        handles = df.handle.unique()\n",
    "        np.random.shuffle(handles)\n",
    "        handles = handles[0:batch_size]\n",
    "\n",
    "        for handle in handles:\n",
    "            tweet_a = get_random_tweet_for_handle(df, handle)\n",
    "            tweet_b = get_random_tweet_for_handle(df, handle, tweet_a)\n",
    "\n",
    "            instance_features = get_instance_features(tweet_a, tweet_b, tokenizer, sequence_length=sequence_length)\n",
    "\n",
    "            for instance_key, instance_value in instance_features.items():\n",
    "                features[instance_key].append(instance_value)\n",
    "\n",
    "        features[\"input_a_sequence\"] = np.array(features[\"input_a_sequence\"], dtype=np.float32)\n",
    "        features[\"input_b_sequence\"] = np.array(features[\"input_b_sequence\"], dtype=np.float32)\n",
    "\n",
    "        yield(features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "When compiling the model, an instance of the loss function must be created and passed as parameter. In this case, I used a dictionary to link the loss function with the named output layer of the model. This can be useful if more than one output layer is specified. Given the definition of this model, tracking accuracy at this stage is not useful: the last layer represents the embeddings of the input sequences. The accuracy of the model will be evaluated later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "steps_per_epoch = 128\n",
    "epochs = 1\n",
    "alpha = 0.25\n",
    "learning_rate = 0.001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss={\n",
    "        \"branches\": define_triplet_loss(\n",
    "            batch_size=batch_size,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    },\n",
    "    metrics=[]\n",
    ")\n",
    "\n",
    "batch_generator = get_generator()\n",
    "\n",
    "history = model.fit(\n",
    "    generator_siamese_network,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "The model defined here can be evaluated in terms of the similarity between the produced embeddings. To compute this metric, we simply have to take the prediction results with respect to an input batch and compute the cosine similarity of the resulting embeddings, after normalization. If everything worked as expected, the resulting matrix will have the higher values (row-wise) in the main diagonal, where values are expected to be close to 1. Other cells hopefully will contain lower values, even negative ones. To make a prediction, one can set and tune a threshold: if the value is over that threshold, then the inputs are predicted as similar, otherwise the result is negative. A second threshold could be used so that values in between the two thresholds can be interpreted as undetermined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model.predict(next(batch_generator))\n",
    "\n",
    "threshold = 0.75\n",
    "\n",
    "def compute_predictions(model_result):\n",
    "    #using normalization, dot product, compute the similarity matrix\n",
    "    return None\n",
    "\n",
    "similarity_matrix_values = compute_predictions(model_result)\n",
    "\n",
    "similarity_decision = similarity_matrix_values >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "The first task (similarity between questions) resulted much easier for the model to learn when compared to the second task (tweet author identification). This can be explained when considering that the text sequences in the question dataset are longer and arguably contain more information than short, possibly unrelated tweets retrieved from the Twitter platform. Also, in this particular setting, emojis and special characters are not considered."
   ]
  }
 ]
}